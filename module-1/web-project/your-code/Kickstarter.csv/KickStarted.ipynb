{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from pandas.io.json import json_normalize \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick=pd.read_csv(\"Kickstarter_projects_Feb19.csv\")\n",
    "kick.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kick.columns)\n",
    "kick.rename(columns={'launched_at': 'inicio_camp', 'deadline': 'fin_camp','goal_usd': 'meta','usd_pledged': 'dinero_reco','currency': 'moneda'}, inplace=True)\n",
    "print(kick.shape)\n",
    "print(kick.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(kick.status.value_counts())\n",
    "kick.dinero_reco.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista donde se almacenara todo el contenido requerido del scrapeo\n",
    "dfidea=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap de Ideame\n",
    "ideamee=[]\n",
    "url = 'https://www.idea.me/projects?categories=%s'\n",
    "pag = 3\n",
    "see_p = 4\n",
    "\n",
    "class Scrappag:\n",
    "\n",
    "    def __init__(self, url_pattern, pages_to_scrape=1, see_more=1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.content_parser = content_parser\n",
    "        self.see_more = see_more\n",
    "         \n",
    "    def scrape_url(self, url):\n",
    "        navegador = webdriver.Chrome()\n",
    "        navegador.get(url)\n",
    "        for u in range(1, self.see_more+1):\n",
    "            navegador.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            see_more = navegador.find_element_by_xpath('/html/body/div[4]/section[4]/div/div/a').click()\n",
    "            time.sleep(3)\n",
    "        pagina_completa = navegador.page_source\n",
    "        response = pagina_completa\n",
    "        time.sleep(1)\n",
    "        result = self.content_parser(response)\n",
    "        self.output_results(result)\n",
    "        navegador.close()\n",
    "        \n",
    "    def output_results(self, r):\n",
    "        dfidea.append(r)\n",
    "        return\n",
    "          \n",
    "    def kickstart(self):\n",
    "        lista=[1,2,4,5,6,8,9,10,13]\n",
    "        for i in lista:\n",
    "            print(\"absorviendo pag: \",i)\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "    \n",
    "def quotes_parser(content):\n",
    "    soup = bs(content, \"html.parser\")\n",
    "    art=soup.find_all('article',{'class':'itemProject standardProject whiteProject'})\n",
    "    \n",
    "    titles=[]\n",
    "    recau=[]\n",
    "    dias=[]\n",
    "    \n",
    "    for u in range(len(art)):\n",
    "        boxtitle=art[u].find_all('div',{'class':'mod-1col title'})\n",
    "        title=boxtitle[0].find(\"h4\")\n",
    "        goal=art[u].find_all('div',{'class':'goal'})\n",
    "        money=goal[0].find('span',{'class':'value'} )\n",
    "        goal=art[u].find_all('div',{'class':'goal'})\n",
    "        por=goal[0].find('ul',{'class':'mod-1col stats'} )\n",
    "        li=por.find(\"li\")\n",
    "        p=li.text\n",
    "        k=re.findall('\\d+',p)\n",
    "        l=money.text\n",
    "        money\n",
    "        x=title.text\n",
    "        titles.append(x)\n",
    "        recau.append(l)\n",
    "        dias.append(k)\n",
    "        \n",
    "    tour={'titulos':titles, 'dias':dias,\n",
    "                'recau':recau}\n",
    "    return tour\n",
    "    \n",
    "ideame = Scrappag(url,pag,see_p, content_parser=quotes_parser)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disparador del scrap\n",
    "ideame.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregando columna de category\n",
    "ArtesV=pd.DataFrame(dfidea[0])\n",
    "ArtesV[\"main_category\"]=\"Artes Visuales\"\n",
    "Dise単o=pd.DataFrame(dfidea[1])\n",
    "Dise単o[\"main_category\"]=\"Dise単o\"\n",
    "Cine=pd.DataFrame(dfidea[2])\n",
    "Cine[\"main_category\"]=\"Cine y Video\"\n",
    "ArtesE=pd.DataFrame(dfidea[3])\n",
    "ArtesE[\"main_category\"]=\"Artes Escenicas\"\n",
    "Musica=pd.DataFrame(dfidea[4])\n",
    "Musica[\"main_category\"]=\"Musica\"\n",
    "Editorial=pd.DataFrame(dfidea[5])\n",
    "Editorial[\"main_category\"]=\"Editorial\"\n",
    "ImpactoS=pd.DataFrame(dfidea[6])\n",
    "ImpactoS[\"main_category\"]=\"Impacto Social\"\n",
    "Educacion=pd.DataFrame(dfidea[7])\n",
    "Educacion[\"main_category\"]=\"Educacion\"\n",
    "Empre=pd.DataFrame(dfidea[8])\n",
    "Empre[\"main_category\"]=\"Emprendimiento\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=ArtesV.append ([Dise単o,Cine,ArtesE,Musica,Editorial,ImpactoS,Educacion,Empre], ignore_index=True)\n",
    "dff.rename(columns={'recau': 'usd_pledged','titulos': 'name'}, inplace=True)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick2=kick.append(dff,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pruebas argentina, no pude aplicar\n",
    "url = 'https://www.idea.me/projects?categories=13'\n",
    "html=requests.get(url).content\n",
    "soup = bs(html, \"html.parser\")\n",
    "art=soup.find_all('article',{'class':'itemProject standardProject whiteProject'})\n",
    "porce=[]\n",
    "for w in range(len(art)):\n",
    "    goal=art[w].find_all('div',{'class':'goal'})\n",
    "    por=goal[0].find('div',{'class':'percent transparent'} )\n",
    "    p=por.text\n",
    "    k=re.findall('\\d+%',p)\n",
    "    porce.append(k)\n",
    "porce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Api Kickstart, la pagina no tenia api oficial por lo que hice request directamente en su URL y maneje los JSON\n",
    "\n",
    "url = \"https://www.kickstarter.com/projects/search.json?search=&term=%s\"\n",
    "\n",
    "class jsonn:\n",
    "\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "         \n",
    "    def scrape_url(self, url):\n",
    "        res=requests.get(url)\n",
    "        response = res.json()\n",
    "        time.sleep(2)\n",
    "        result = self.content_parser(response)\n",
    "        self.output_results(result)\n",
    "        \n",
    "    def output_results(self, r):\n",
    "        dfArt=r[[\"name\",'country','category.name',\"goal\",\"state\",\"currency\",\"pledged\"]]\n",
    "        archivo= \"Kick.csv\"\n",
    "        txt=dfArt.to_csv(index=False)\n",
    "        csv=open(archivo,\"a\")\n",
    "        csv.write(txt)\n",
    "        \n",
    "        return \n",
    "          \n",
    "    def kickstart(self):\n",
    "        lista=[\"Arte\",\"Comic\",\"Crafts\",\"Dance\",\"Technology\",\"Games\",\"Food\",\"Film\",\"Fashion\"]\n",
    "        for i in lista:\n",
    "            print(\"Get category: \",i)\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "    \n",
    "def quotes_parser(content):\n",
    "    df1=json_normalize(content[\"projects\"])\n",
    "    \n",
    "    return df1\n",
    "    \n",
    "kick = jsonn(url, content_parser=quotes_parser)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get category:  Arte\n",
      "Get category:  Comic\n",
      "Get category:  Crafts\n",
      "Get category:  Dance\n",
      "Get category:  Technology\n",
      "Get category:  Games\n",
      "Get category:  Food\n",
      "Get category:  Film\n",
      "Get category:  Fashion\n"
     ]
    }
   ],
   "source": [
    "#Disparador Api\n",
    "kick.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xed in position 6: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xed in position 6: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-07316fe4a385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Kick.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkii\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category.name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkickfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkick2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xed in position 6: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "kii=pd.read_csv(\"Kick.csv\")\n",
    "kii['category.name'].value_counts()\n",
    "kickfinal=kick2.append(kii)\n",
    "kickfinal.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
